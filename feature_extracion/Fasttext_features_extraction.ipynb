{"cells":[{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22347,"status":"ok","timestamp":1735831677780,"user":{"displayName":"Saeed Ahmed","userId":"16724720564507897686"},"user_tz":-300},"id":"Bqo1DwpU-G1Y","outputId":"a3cb966e-c0ca-44c3-a0c4-3eb8862324f1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7469,"status":"ok","timestamp":1726681829713,"user":{"displayName":"Saeed Ahmed","userId":"16724720564507897686"},"user_tz":-120},"id":"sIEcAwS8-O73","outputId":"74071c87-a8c1-4967-f0aa-648cb5712ee2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.0)\n","Requirement already satisfied: huggingface-hub\u003c1.0,\u003e=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n","Requirement already satisfied: numpy\u003e=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n","Requirement already satisfied: packaging\u003e=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n","Requirement already satisfied: pyyaml\u003e=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: safetensors\u003e=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n","Requirement already satisfied: tokenizers\u003c0.20,\u003e=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n","Requirement already satisfied: tqdm\u003e=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n","Requirement already satisfied: fsspec\u003e=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub\u003c1.0,\u003e=0.23.2-\u003etransformers) (2024.6.1)\n","Requirement already satisfied: typing-extensions\u003e=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub\u003c1.0,\u003e=0.23.2-\u003etransformers) (4.12.2)\n","Requirement already satisfied: charset-normalizer\u003c4,\u003e=2 in /usr/local/lib/python3.10/dist-packages (from requests-\u003etransformers) (3.3.2)\n","Requirement already satisfied: idna\u003c4,\u003e=2.5 in /usr/local/lib/python3.10/dist-packages (from requests-\u003etransformers) (3.10)\n","Requirement already satisfied: urllib3\u003c3,\u003e=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests-\u003etransformers) (2.0.7)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests-\u003etransformers) (2024.8.30)\n"]}],"source":["!pip install transformers"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2823,"status":"ok","timestamp":1735831734680,"user":{"displayName":"Saeed Ahmed","userId":"16724720564507897686"},"user_tz":-300},"id":"srRm3d8B-P3s"},"outputs":[{"name":"stdout","output_type":"stream","text":["Features extracted and saved to CSV successfully.\n","[[ 1.          1.          1.         ... -0.112975    0.04777413\n","   0.03818732]\n"," [ 5.          3.          4.         ... -0.11297341  0.04729144\n","   0.042171  ]\n"," [ 1.          0.          2.         ... -0.11337797  0.04756185\n","   0.03980466]\n"," ...\n"," [ 4.          1.          0.         ... -0.1154198   0.04698378\n","   0.04280837]\n"," [ 4.          1.          0.         ... -0.1154198   0.04698378\n","   0.04280837]\n"," [ 4.          1.          0.         ... -0.1154198   0.04698378\n","   0.04280837]]\n"]}],"source":["import os\n","import re\n","import sys\n","import pandas as pd\n","import numpy as np\n","from gensim.models import Word2Vec\n","from collections import Counter\n","\n","\n","def read_peptide_sequences(file):\n","    if not os.path.exists(file):\n","        print(f'Error: file {file} does not exist.')\n","        sys.exit(1)\n","\n","    with open(file) as f:\n","        records = f.read()\n","\n","    if '\u003e' not in records:\n","        print(f'Error: the input file {file} seems not in FASTA format!')\n","        sys.exit(1)\n","\n","    records = records.split('\u003e')[1:]\n","    peptide_sequences = []\n","    for fasta in records:\n","        array = fasta.split('\\n')\n","        header, sequence = array[0], ''.join(array[1:]).upper()\n","        peptide_sequences.append(sequence)\n","\n","    return peptide_sequences\n","\n","def extract_features(peptide_sequences, vector_size=100, window=5, min_count=1):\n","    # Prepare data for Word2Vec\n","    tokenized_sequences = [list(sequence) for sequence in peptide_sequences]\n","\n","    # Train Word2Vec model\n","    model = Word2Vec(tokenized_sequences, vector_size=vector_size, window=window, min_count=min_count)\n","\n","    # Create a vocabulary list\n","    vocabulary = list(model.wv.index_to_key)\n","\n","    # Extract BoW + Word2Vec features\n","    features = []\n","    for sequence in tokenized_sequences:\n","        # Bag of Words representation\n","        bow = Counter(sequence)\n","        bow_vector = [bow[token] for token in vocabulary]\n","\n","        # Word2Vec representation\n","        word2vec_vector = np.zeros(vector_size)\n","        for token in sequence:\n","            if token in model.wv:\n","                word2vec_vector += model.wv[token]\n","        word2vec_vector /= len(sequence)\n","\n","        # Combine BoW and Word2Vec vectors\n","        combined_vector = np.concatenate([bow_vector, word2vec_vector])\n","        features.append(combined_vector)\n","\n","    return np.array(features), vocabulary\n","def load_sequences_from_csv(file_path, column_name):\n","  \"\"\"\n","  Load sequences from a specific column in a CSV file.\n","  :param file_path: Path to the CSV file.\n","  :param column_name: Name of the column containing the sequences.\n","  :return: List of sequences.\n","  \"\"\"\n","  df = pd.read_csv(file_path)\n","  if column_name not in df.columns:\n","      raise ValueError(f\"Column '{column_name}' not found in the CSV file.\")\n","  return df[column_name].dropna().astype(str).tolist()\n","\n","def main():\n","    # File paths\n","    # path = '/content/drive/MyDrive/Watashara_Projects/TIP/'\n","    # file_path = path+'Features_extraction/TR_IND_Pos_Neg.fasta'\n","    # output_csv = path + 'features/Fasttext_features_TIP.csv'\n","\n","    # # Read peptide sequences\n","    # peptide_sequences = read_peptide_sequences(file_path)\n","    path = '/content/drive/MyDrive/Watashara_Projects/8-Dengue/'\n","    csv_file = path + \"DENV_AbDataset.csv\"  # Path to your CSV file\n","    column_name = \"Antibody_Sequence\"   #Antibody_Sequence   Epitope_Sequence    # Replace with the actual column name containing sequence\n","    output_csv = path + 'features/Fasttext_Antibody.csv'\n","\n","    # Load sequences from the CSV file\n","    sequences = load_sequences_from_csv(csv_file, column_name)\n","\n","    # Extract features using BoW + Word2Vec\n","    features, vocabulary = extract_features(sequences)\n","\n","    # Create a DataFrame with the combined features\n","    bow_columns = [f'bow_{token}' for token in vocabulary]\n","    word2vec_columns = [f'word2vec_{i}' for i in range(features.shape[1] - len(vocabulary))]\n","    columns = bow_columns + word2vec_columns\n","\n","    features_df = pd.DataFrame(features, columns=columns)\n","    features_df.to_csv(output_csv, index=False)\n","\n","    print(\"Features extracted and saved to CSV successfully.\")\n","    print(features)\n","\n","if __name__ == \"__main__\":\n","    main()\n"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyMR+UDKmQ8zsp9p6An1jmbC","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}