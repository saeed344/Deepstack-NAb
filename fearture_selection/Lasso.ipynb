{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 46142,
     "status": "ok",
     "timestamp": 1736570800787,
     "user": {
      "displayName": "Saeed Ahmed",
      "userId": "16724720564507897686"
     },
     "user_tz": -300
    },
    "id": "xrAABewoHOkt",
    "outputId": "6433d319-96f8-4cc7-c01f-dd75372ea02d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18929,
     "status": "ok",
     "timestamp": 1736570819713,
     "user": {
      "displayName": "Saeed Ahmed",
      "userId": "16724720564507897686"
     },
     "user_tz": -300
    },
    "id": "P3iIt9jgWdCi",
    "outputId": "06ea13f0-a179-4d91-8c8d-56c317075839"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1108, 20)\n",
      "(1108, 531)\n",
      "(1108, 22)\n",
      "(1108, 39)\n",
      "(1108, 195)\n",
      "(1108, 400)\n",
      "(1108, 20)\n",
      "(1108, 578)\n",
      "(1108, 50)\n",
      "(1108, 20)\n",
      "(1108, 219)\n",
      "(1108, 767)\n",
      "(1108, 20)\n",
      "(1108, 531)\n",
      "(1108, 22)\n",
      "(1108, 39)\n",
      "(1108, 195)\n",
      "(1108, 400)\n",
      "(1108, 20)\n",
      "(1108, 578)\n",
      "(1108, 50)\n",
      "(1108, 20)\n",
      "(1108, 220)\n",
      "(1108, 767)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load features\n",
    "path = \"/content/drive/MyDrive/Watashara_Projects/8-Dengue/features/\"\n",
    "\n",
    "AAC_Antibody = pd.read_csv(path + 'AAC_Antibody.csv').iloc[:, 1:].values\n",
    "AAI_Antibody = pd.read_csv(path + 'AAINDEX_Antibody.csv').iloc[:, 1:].values\n",
    "APAAC_Antibody = pd.read_csv(path + 'APAAC_Antibody.csv').iloc[:, 1:].values\n",
    "CTDC_Antibody = pd.read_csv(path + 'CTDC_Antibody.csv').iloc[:, 1:].values\n",
    "CTDD_Antibody = pd.read_csv(path + 'CTDD_Antibody.csv').iloc[:,1:].values\n",
    "DPC_Antibody = pd.read_csv(path + 'DPC_Antibody.csv').iloc[:, 1:].values\n",
    "EAAC_Antibody = pd.read_csv(path + 'EAAC_Antibody.csv').iloc[:, 1:].values\n",
    "FEGS_Antibody = pd.read_csv(path + 'FEGS_Antibody.csv').iloc[:, :].values\n",
    "DCGR_Antibody = pd.read_csv(path + 'DCGR_Antibody_features.csv').iloc[:, :].values\n",
    "BLOSUM62_Antibody = pd.read_csv(path + 'BLOSUM62_Antibody.csv').iloc[:, 1:].values\n",
    "word2vec_Antibody = pd.read_csv(path + 'Antibody_word2vec.csv').iloc[:, 1:].values\n",
    "bert_Antibody = pd.read_csv(path + 'Antibody_bert.csv').iloc[:, 1:].values\n",
    "# Fasttex_Antibody = pd.read_csv(path + 'Fasttext_Antibody.csv').iloc[:, 1:].values\n",
    "\n",
    "AAC_Epitope = pd.read_csv(path + 'AAC_Epitope.csv').iloc[:, 1:].values\n",
    "AAI_Epitope = pd.read_csv(path + 'AAINDEX_Epitope.csv').iloc[:, 1:].values\n",
    "APAAC_Epitope = pd.read_csv(path + 'APAAC_Epitope.csv').iloc[:, 1:].values\n",
    "CTDC_Epitope = pd.read_csv(path + 'CTDC_Epitope.csv').iloc[:, 1:].values\n",
    "CTDD_Epitope = pd.read_csv(path + 'CTDD_Epitope.csv').iloc[:,1:].values\n",
    "DPC_Epitope = pd.read_csv(path + 'DPC_Epitope.csv').iloc[:, 1:].values\n",
    "EAAC_Epitope = pd.read_csv(path + 'EAAC_Epitope.csv').iloc[:, 1:].values\n",
    "FEGS_Epitope = pd.read_csv(path + 'FEGS_Epitope.csv').iloc[:, :].values\n",
    "DCGR_Epitope = pd.read_csv(path + 'DCGR_Epitope_features.csv').iloc[:, :].values\n",
    "BLOSUM62_Epitope = pd.read_csv(path + 'BLOSUM62_Epitope.csv').iloc[:, 1:].values\n",
    "word2vec_Epitope = pd.read_csv(path + 'Epitope_word2vec.csv').iloc[:, 1:].values\n",
    "bert_Epitope = pd.read_csv(path + 'Epitope_bert.csv').iloc[:, 1:].values\n",
    "# Fasttext_Epitope = pd.read_csv(path + 'Fasttext_Epitope.csv').iloc[:, 1:].values\n",
    "\n",
    "print(np.shape(AAC_Antibody))\n",
    "print(np.shape(AAI_Antibody))\n",
    "print(np.shape(APAAC_Antibody))\n",
    "print(np.shape(CTDC_Antibody))\n",
    "print(np.shape(CTDD_Antibody))\n",
    "print(np.shape(DPC_Antibody))\n",
    "print(np.shape(EAAC_Antibody))\n",
    "print(np.shape(FEGS_Antibody))\n",
    "print(np.shape(DCGR_Antibody))\n",
    "print(np.shape(BLOSUM62_Antibody))\n",
    "print(np.shape(word2vec_Antibody))\n",
    "print(np.shape(bert_Antibody))\n",
    "# print(np.shape(Fasttex_Antibody))\n",
    "\n",
    "# Combine all features\n",
    "# all_train_features = np.column_stack((AAC,AAI,ASDC, CTDC,CTDD, CTDT, DDE, onehot,OPF, Train_FEGS,word2vec,bert))#, KR, KRC, PubChem, RDKIT\n",
    "\n",
    "\n",
    "print(np.shape(AAC_Epitope))\n",
    "print(np.shape(AAI_Epitope))\n",
    "print(np.shape(APAAC_Epitope))\n",
    "print(np.shape(CTDC_Epitope))\n",
    "print(np.shape(CTDD_Epitope))\n",
    "print(np.shape(DPC_Epitope))\n",
    "print(np.shape(EAAC_Epitope))\n",
    "print(np.shape(FEGS_Epitope))\n",
    "print(np.shape(DCGR_Epitope))\n",
    "print(np.shape(BLOSUM62_Epitope))\n",
    "print(np.shape(word2vec_Epitope))\n",
    "print(np.shape(bert_Epitope))\n",
    "# print(np.shape(Fasttext_Epitope))\n",
    "# Combine all features\n",
    "AAC = np.column_stack((AAC_Antibody,AAC_Epitope))\n",
    "# AAI = np.column_stack((AAI_Antibody, AAI_Epitope))\n",
    "APAAC = np.column_stack((APAAC_Antibody,APAAC_Epitope))\n",
    "CTDC = np.column_stack((CTDC_Antibody,CTDC_Epitope))\n",
    "# CTDD = np.column_stack((CTDD_Antibody,CTDD_Epitope))\n",
    "# DPC = np.column_stack((DPC_Antibody,DPC_Epitope))\n",
    "# EAAC = np.column_stack((EAAC_Antibody,EAAC_Epitope))\n",
    "FEGS = np.column_stack((FEGS_Antibody,FEGS_Epitope))\n",
    "DCGR = np.column_stack((DCGR_Antibody,DCGR_Epitope))\n",
    "BLOSUM62 = np.column_stack((BLOSUM62_Antibody,BLOSUM62_Epitope))\n",
    "word2vec = np.column_stack((word2vec_Antibody,word2vec_Epitope))\n",
    "bert = np.column_stack((bert_Antibody,bert_Epitope))\n",
    "# Fasttext = np.column_stack((Fasttex_Antibody,Fasttext_Epitope))\n",
    "\n",
    "\n",
    "all_features = np.column_stack((AAC_Antibody,AAC_Epitope,APAAC_Antibody, APAAC_Epitope, CTDC_Antibody, CTDC_Epitope,\n",
    "                                    FEGS_Antibody,FEGS_Epitope,DCGR_Antibody,DCGR_Epitope,BLOSUM62_Antibody,\n",
    "                                BLOSUM62_Epitope,word2vec_Antibody,word2vec_Epitope,bert_Antibody,bert_Epitope))\n",
    "\n",
    "# print(np.shape(all_features))\n",
    "\n",
    "# labela=np.ones((554 ,1))  #Value can be changed\n",
    "# labelb=np.zeros((554,1))\n",
    "# labels=np.append(labela,labelb)\n",
    "# y = labels\n",
    "# X_train,X_test,y_train,y_test = train_test_split(all_features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# print(np.shape(X_train))\n",
    "# print(np.shape(y_train))\n",
    "# print(np.shape(X_test))\n",
    "# print(np.shape(y_test))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18750,
     "status": "ok",
     "timestamp": 1736404946881,
     "user": {
      "displayName": "Saeed Ahmed",
      "userId": "16724720564507897686"
     },
     "user_tz": -300
    },
    "id": "-fxB63doJyda",
    "outputId": "f29c6e5b-4426-4c2e-ca68-49a06043723b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lightning\n",
      "  Downloading lightning-2.5.0.post0-py3-none-any.whl.metadata (40 kB)\n",
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/40.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.4/40.4 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: PyYAML<8.0,>=5.4 in /usr/local/lib/python3.10/dist-packages (from lightning) (6.0.2)\n",
      "Requirement already satisfied: fsspec<2026.0,>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning) (2024.10.0)\n",
      "Collecting lightning-utilities<2.0,>=0.10.0 (from lightning)\n",
      "  Downloading lightning_utilities-0.11.9-py3-none-any.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: packaging<25.0,>=20.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (24.2)\n",
      "Requirement already satisfied: torch<4.0,>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (2.5.1+cu121)\n",
      "Collecting torchmetrics<3.0,>=0.7.0 (from lightning)\n",
      "  Downloading torchmetrics-1.6.1-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: tqdm<6.0,>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<6.0,>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (4.12.2)\n",
      "Collecting pytorch-lightning (from lightning)\n",
      "  Downloading pytorch_lightning-2.5.0.post0-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning) (3.11.11)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities<2.0,>=0.10.0->lightning) (75.1.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.1.0->lightning) (3.16.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.1.0->lightning) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.1.0->lightning) (3.1.5)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.1.0->lightning) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch<4.0,>=2.1.0->lightning) (1.3.0)\n",
      "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics<3.0,>=0.7.0->lightning) (1.26.4)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (4.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.18.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<4.0,>=2.1.0->lightning) (3.0.2)\n",
      "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (3.10)\n",
      "Downloading lightning-2.5.0.post0-py3-none-any.whl (815 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m815.2/815.2 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading lightning_utilities-0.11.9-py3-none-any.whl (28 kB)\n",
      "Downloading torchmetrics-1.6.1-py3-none-any.whl (927 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m927.3/927.3 kB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pytorch_lightning-2.5.0.post0-py3-none-any.whl (819 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m819.3/819.3 kB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: lightning-utilities, torchmetrics, pytorch-lightning, lightning\n",
      "Successfully installed lightning-2.5.0.post0 lightning-utilities-0.11.9 pytorch-lightning-2.5.0.post0 torchmetrics-1.6.1\n",
      "Collecting sklearn-contrib-lightning\n",
      "  Downloading sklearn_contrib_lightning-0.6.2.post0-cp310-cp310-manylinux2014_x86_64.whl.metadata (4.5 kB)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sklearn-contrib-lightning) (1.4.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sklearn-contrib-lightning) (1.26.4)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sklearn-contrib-lightning) (1.6.0)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sklearn-contrib-lightning) (1.13.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sklearn-contrib-lightning) (3.5.0)\n",
      "Downloading sklearn_contrib_lightning-0.6.2.post0-cp310-cp310-manylinux2014_x86_64.whl (3.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sklearn-contrib-lightning\n",
      "Successfully installed sklearn-contrib-lightning-0.6.2.post0\n",
      "Collecting torch_geometric\n",
      "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.11.11)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2024.10.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.5)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.26.4)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (5.9.5)\n",
      "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.2.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (4.67.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (4.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.18.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2024.12.14)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp->torch_geometric) (4.12.2)\n",
      "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: torch_geometric\n",
      "Successfully installed torch_geometric-2.6.1\n",
      "ERROR: unknown command \"dgl\"\n"
     ]
    }
   ],
   "source": [
    "!pip install lightning\n",
    "!pip install sklearn-contrib-lightning\n",
    "!pip install torch_geometric\n",
    "!pip dgl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZR6KaLMRWw_K"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UNV9mUZEWnq2"
   },
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import scale,StandardScaler\n",
    "\n",
    "def Light_lasso(X,y,alpha_):\n",
    "     clf = CDClassifier(penalty=\"l1/l2\",\n",
    "                   loss=\"squared_hinge\",\n",
    "                   #multiclass=True,\n",
    "                   max_iter=50,\n",
    "                   alpha=alpha_,\n",
    "                   C=1.0 / X.shape[0],\n",
    "                   tol=1e-3)\n",
    "     clf.fit(X, y  )\n",
    "     H1,H2=np.nonzero(clf.coef_)\n",
    "     X=X[:,H2]\n",
    "     return X,H2\n",
    "\n",
    "path = \"/content/drive/MyDrive/Watashara_Projects/8-Dengue/\"\n",
    "\n",
    "train_data=pd.read_csv(path + 'features/Train_clean.csv')\n",
    "data=np.array(train_data)\n",
    "data=data[:,:]\n",
    "[m1,n1]=np.shape(data)\n",
    "label1=np.ones((554 ,1))#Value can be changed\n",
    "label2=np.zeros((554 ,1))\n",
    "labels=np.append(label1,label2)\n",
    "y = labels\n",
    "# test_data=pd.read_csv(path + 'features/Test_clean.csv')\n",
    "# data_test=np.array(test_data)\n",
    "# test_data=data_test[:,:]\n",
    "\n",
    "\n",
    "# shu=scale(data)\n",
    "X=data\n",
    "y=labels\n",
    "# ata_2,importance=Light_lasso(X,y.T.ravel(),0.03)\n",
    "data_2,importance=Light_lasso(X,y,0.03)\n",
    "# Select the same features from test data using the importance indices\n",
    "# test_data_selected = test_data[:, importance]\n",
    "\n",
    "# Save the selected features for training and testing\n",
    "train_data_csv = pd.DataFrame(data=data_2)\n",
    "train_data_csv.to_csv(path + 'Feature_Selection/GLasso_feat.csv', index=False)\n",
    "\n",
    "# test_data_csv = pd.DataFrame(data=test_data_selected)\n",
    "# test_data_csv.to_csv(path + 'Feature_Selection/GLasso_Test_feat.csv', index=False)\n",
    "\n",
    "# Save the feature importance indices\n",
    "importance_csv = pd.DataFrame(data=importance)\n",
    "importance_csv.to_csv(path + 'Feature_Selection/GLasso_feat_importance.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JbA-ZQMaXAdA"
   },
   "source": [
    "RP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xEmfpkT7W-mC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yTGMzZABW_hh"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import random_projection\n",
    "from sklearn.preprocessing import scale\n",
    "import pandas as pd\n",
    "\n",
    "# Load train data\n",
    "path = \"/content/drive/MyDrive/Watashara_Projects/8-Dengue/\"\n",
    "\n",
    "train_data=pd.read_csv(path + 'features/Train_clean.csv')\n",
    "data=np.array(train_data)\n",
    "data=data[:,:]\n",
    "[m1,n1]=np.shape(data)\n",
    "label1=np.ones((554 ,1))#Value can be changed\n",
    "label2=np.zeros((554 ,1))\n",
    "labels=np.append(label1,label2)\n",
    "y = labels\n",
    "\n",
    "# Load test data\n",
    "# test_data = pd.read_csv(path + 'Feature_Selection/ind_data_prot_GRU1500.csv')\n",
    "# data_test = np.array(test_data)\n",
    "# X_test = data_test[:, :]  # Use all columns for features\n",
    "# label_test1 = np.ones((235, 1))  # Adjust size for positive test labels\n",
    "# label_test2 = np.zeros((235, 1))  # Adjust size for negative test labels\n",
    "# y_test = np.append(label_test1, label_test2)\n",
    "\n",
    "# Optional: Scale data (uncomment if needed)\n",
    "# X_train = scale(X_train)\n",
    "# X_test = scale(X_test)\n",
    "\n",
    "# Apply random projection to train data\n",
    "rp = random_projection.SparseRandomProjection(n_components=300, random_state=66)\n",
    "X_train_projected = rp.fit_transform(data)\n",
    "# X_test_projected = rp.transform(X_test)\n",
    "\n",
    "\n",
    "data_csv = pd.DataFrame(data=X_train_projected)\n",
    "data_csv.to_csv(path + 'Feature_Selection/RP_feat.csv')\n",
    "# data_csv = pd.DataFrame(data=X_test_projected)\n",
    "# data_csv.to_csv(path + 'Feature_Selection/RP_test_prot_GRU1500.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c24A8mhuXNjD"
   },
   "source": [
    "**TSVD**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "",
   "version": ""
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
